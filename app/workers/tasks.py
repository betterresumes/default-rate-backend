import logging
import math
import time
import traceback
import uuid
import json
import io
import base64
import pandas as pd
from datetime import datetime
from typing import Dict, Any, List, Optional
from celery import current_task
from functools import wraps

from ..workers.celery_app import celery_app
from ..core.database import get_session_local, BulkUploadJob, Company, AnnualPrediction, QuarterlyPrediction, User, Organization
from ..services.ml_service import ml_model
from ..services.quarterly_ml_service import quarterly_ml_model

logger = logging.getLogger(__name__)


def safe_float(value):
    """Convert value to float, handling None and NaN values"""
    if value is None:
        return 0
    try:
        float_val = float(value)
        if math.isnan(float_val) or math.isinf(float_val):
            return 0
        return float_val
    except (ValueError, TypeError):
        return 0


def update_job_status(
    job_id: str,
    status: str,
    processed_rows: Optional[int] = None,
    successful_rows: Optional[int] = None,
    failed_rows: Optional[int] = None,
    error_message: Optional[str] = None,
    error_details: Optional[Dict] = None
):
    """Update job status in database"""
    SessionLocal = get_session_local()
    db = SessionLocal()
    
    try:
        job = db.query(BulkUploadJob).filter(BulkUploadJob.id == job_id).first()
        if not job:
            return
        
        job.status = status
        
        if processed_rows is not None:
            job.processed_rows = processed_rows
        if successful_rows is not None:
            job.successful_rows = successful_rows
        if failed_rows is not None:
            job.failed_rows = failed_rows
        if error_message is not None:
            job.error_message = error_message
        if error_details is not None:
            job.error_details = json.dumps(error_details)
        
        if status == 'processing' and job.started_at is None:
            job.started_at = datetime.utcnow()
        elif status in ['completed', 'failed']:
            job.completed_at = datetime.utcnow()
        
        db.commit()
        
    except Exception as e:
        db.rollback()
        logger.error(f"Error updating job status: {str(e)}")
    finally:
        db.close()


def create_or_get_company(db, symbol: str, name: str, market_cap: float, sector: str, organization_id: Optional[str], user_id: str) -> Company:
    """Create or get company with organization scoping"""
    print(f"[COMPANY-DEBUG] Starting create_or_get_company for {symbol}")
    
    if organization_id:
        print(f"[COMPANY-DEBUG] Using organization scope: {organization_id}")
        access_level = "organization"
        print(f"[COMPANY-DEBUG] About to query company with organization")
        company = db.query(Company).filter(
            Company.symbol == symbol.upper(),
            Company.organization_id == organization_id,
            Company.access_level == "organization"
        ).first()
        print(f"[COMPANY-DEBUG] Organization company query completed: {'Found' if company else 'Not found'}")
    else:
        print(f"[COMPANY-DEBUG] No organization, checking user: {user_id}")
        print(f"[COMPANY-DEBUG] About to query user")
        user = db.query(User).filter(User.id == user_id).first()
        print(f"[COMPANY-DEBUG] User query completed: {'Found' if user else 'Not found'}")
        
        if user and user.role == "super_admin":
            print(f"[COMPANY-DEBUG] User is super_admin")
            access_level = "system"
            print(f"[COMPANY-DEBUG] About to query system company")
            company = db.query(Company).filter(
                Company.symbol == symbol.upper(),
                Company.access_level == "system"
            ).first()
            print(f"[COMPANY-DEBUG] System company query completed: {'Found' if company else 'Not found'}")
        else:
            print(f"[COMPANY-DEBUG] User is regular user or not found")
            access_level = "personal"
            print(f"[COMPANY-DEBUG] About to query personal company")
            company = db.query(Company).filter(
                Company.symbol == symbol.upper(),
                Company.organization_id.is_(None),
                Company.access_level == "personal",
                Company.created_by == user_id
            ).first()
            print(f"[COMPANY-DEBUG] Personal company query completed: {'Found' if company else 'Not found'}")
    
    if not company:
        print(f"[COMPANY-DEBUG] No existing company found, creating new one")
        company = Company(
            symbol=symbol.upper(),
            name=name,
            market_cap=safe_float(market_cap),  # Market cap already in millions
            sector=sector,
            organization_id=organization_id,
            access_level=access_level,
            created_by=user_id
        )
        print(f"[COMPANY-DEBUG] Company object created, about to add to DB")
        db.add(company)
        print(f"[COMPANY-DEBUG] Company added to DB, about to flush")
        db.flush()  
        print(f"[COMPANY-DEBUG] DB flush completed")
    else:
        print(f"[COMPANY-DEBUG] Updating existing company")
        company.name = name
        company.market_cap = safe_float(market_cap)  # Market cap already in millions
        company.sector = sector
        print(f"[COMPANY-DEBUG] Company update completed")
    
    print(f"[COMPANY-DEBUG] create_or_get_company completed for {symbol}")
    return company


# ENHANCED LOGGING SYSTEM FOR WORKER TASKS
class TaskLogger:
    """Enhanced logging for Celery tasks with job and user context"""
    
    def __init__(self, task_name: str = None):
        self.task_name = task_name or "unknown"
        self.start_time = datetime.utcnow()
        
    def log(self, level: str, message: str, **kwargs):
        """Log with enhanced context"""
        timestamp = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
        task_id = getattr(current_task.request, 'id', 'no-task-id') if current_task else 'no-task'
        
        # Extract context from kwargs
        job_id = kwargs.get('job_id', 'no-job-id')
        user_id = kwargs.get('user_id', 'no-user')
        file_name = kwargs.get('file_name', 'unknown-file')
        total_rows = kwargs.get('total_rows', 0)
        processed_rows = kwargs.get('processed_rows', 0)
        queue_priority = kwargs.get('queue_priority', 'medium')
        
        # Calculate processing stats
        elapsed_time = (datetime.utcnow() - self.start_time).total_seconds()
        rows_per_second = processed_rows / elapsed_time if elapsed_time > 0 and processed_rows > 0 else 0
        
        # Format enhanced log message
        log_message = (
            f"[{timestamp}] [{level}] [TASK:{self.task_name}] [ID:{task_id}] "
            f"[JOB:{job_id}] [USER:{user_id}] [FILE:{file_name}] "
            f"[ROWS:{processed_rows}/{total_rows}] [QUEUE:{queue_priority}] "
            f"[RATE:{rows_per_second:.1f}/s] [TIME:{elapsed_time:.1f}s] {message}"
        )
        
        print(log_message)
        
        # Also log to standard logger
        logger_method = getattr(logger, level.lower(), logger.info)
        logger_method(f"[{self.task_name}] {message}", extra={
            'task_id': task_id,
            'job_id': job_id,
            'user_id': user_id,
            'file_name': file_name,
            'total_rows': total_rows,
            'processed_rows': processed_rows,
            'queue_priority': queue_priority,
            'rows_per_second': rows_per_second,
            'elapsed_time': elapsed_time
        })
    
    def info(self, message: str, **kwargs):
        self.log('INFO', message, **kwargs)
        
    def error(self, message: str, **kwargs):
        self.log('ERROR', message, **kwargs)
    
    def log_task_start(self, message: str, **kwargs):
        """Log task start with full context"""
        self.info(f"ðŸš€ Task started")
        self.info(message, **kwargs)
        
    def log_progress(self, message: str, **kwargs):
        """Log progress updates"""
        self.info(message, **kwargs)
        
    def log_completion(self, message: str, **kwargs):
        """Log task completion"""
        self.success(message, **kwargs)
        
    def warning(self, message: str, **kwargs):
        self.log('WARN', message, **kwargs)
        
    def success(self, message: str, **kwargs):
        self.log('SUCCESS', message, **kwargs)


def enhanced_task_logging(task_name: str = None):
    """Decorator to add enhanced logging to Celery tasks"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Extract context for logging
            job_id = kwargs.get('job_id', args[0] if args else 'unknown')
            user_id = kwargs.get('user_id', 'unknown')
            
            task_logger = TaskLogger(task_name or func.__name__)
            
            # Log task start
            task_logger.info(f"ðŸš€ Task started", 
                           job_id=job_id, 
                           user_id=user_id,
                           **kwargs)
            
            try:
                # Execute the task
                result = func(*args, **kwargs)
                
                # Log successful completion
                task_logger.success(f"âœ… Task completed successfully", 
                                  job_id=job_id, 
                                  user_id=user_id,
                                  **kwargs)
                return result
                
            except Exception as e:
                # Log error with full context
                task_logger.error(f"âŒ Task failed: {str(e)}", 
                                job_id=job_id, 
                                user_id=user_id,
                                error=str(e),
                                **kwargs)
                
                # Log stack trace for debugging
                task_logger.error(f"ðŸ“‹ Stack trace: {traceback.format_exc()}", 
                                job_id=job_id, 
                                user_id=user_id,
                                **kwargs)
                raise
                
        return wrapper
    return decorator


@celery_app.task(bind=True, name="app.workers.tasks.process_annual_bulk_upload_task")
@enhanced_task_logging("process_annual_bulk_upload_task")
def process_annual_bulk_upload_task(
    self, 
    job_id: str, 
    data: List[Dict[str, Any]], 
    user_id: str, 
    organization_id: Optional[str]
) -> Dict[str, Any]:
    """
    Enhanced Celery task to process annual predictions bulk upload with comprehensive logging
    
    Args:
        job_id: Bulk upload job ID
        data: List of row data from Excel/CSV
        user_id: ID of user who initiated upload
        organization_id: Organization ID (None for super admin global uploads)
        
    Returns:
        Dictionary with processing results
    """
    task_id = self.request.id
    start_time = time.time()
    
    # Initialize enhanced logger
    task_logger = TaskLogger("process_annual_bulk_upload_task")
    
    # Get job details for logging
    SessionLocal = get_session_local()
    db = SessionLocal()
    
    try:
        job = db.query(BulkUploadJob).filter(BulkUploadJob.id == job_id).first()
        file_name = job.original_filename if job else "unknown-file"
        total_rows = len(data)
        
        # Determine queue priority based on file size
        if total_rows <= 100:
            queue_priority = "high"
        elif total_rows <= 1000:
            queue_priority = "medium"
        else:
            queue_priority = "low"
        
        # Log detailed task start
        task_logger.info(
            f"ðŸ“Š Starting annual bulk upload processing",
            job_id=job_id,
            user_id=user_id,
            file_name=file_name,
            total_rows=total_rows,
            queue_priority=queue_priority,
            organization_id=organization_id or "global"
        )
        
        # Update job status
        update_job_status(job_id, 'processing')
        
        # Initialize caches for performance optimization
        user_cache = {}  # Cache user lookups
        company_cache = {}  # Cache company lookups
        
        successful_rows = 0
        failed_rows = 0
        error_details = []
        
        # Progress tracking for large files
        progress_interval = max(1, total_rows // 20)  # Report progress every 5%
        next_progress_report = progress_interval
        
        # Main processing loop with enhanced logging
        try:
            self.update_state(
                state="PROGRESS",
                meta={
                    "status": "Processing annual predictions...",
                    "current": 0,
                    "total": total_rows,
                    "job_id": job_id
                }
            )
            
            for i, row in enumerate(data):
                try:
                    # Progress reporting with detailed logging
                    if i >= next_progress_report or i == total_rows - 1:
                        progress_percent = (i / total_rows) * 100
                        elapsed_time = time.time() - start_time
                        rows_per_second = i / elapsed_time if elapsed_time > 0 else 0
                        estimated_remaining = ((total_rows - i) / rows_per_second) if rows_per_second > 0 else 0
                        
                        task_logger.info(
                            f"ðŸ“ˆ Processing progress: {progress_percent:.1f}% ({i}/{total_rows} rows)",
                            job_id=job_id,
                            user_id=user_id,
                            file_name=file_name,
                            total_rows=total_rows,
                            processed_rows=i,
                            queue_priority=queue_priority,
                            successful_rows=successful_rows,
                            failed_rows=failed_rows,
                            estimated_remaining_seconds=estimated_remaining
                        )
                        next_progress_report = min(i + progress_interval, total_rows)
                        
                        # Update job progress
                        update_job_status(
                            job_id, 
                            'processing', 
                            processed_rows=i,
                            successful_rows=successful_rows,
                            failed_rows=failed_rows
                        )
                    
                    # Process individual row with error resilience
                    try:
                        company = create_or_get_company_cached(
                            db=db,
                            symbol=row['company_symbol'],
                            name=row['company_name'],
                            market_cap=safe_float(row['market_cap']),
                            sector=row['sector'],
                            organization_id=organization_id,
                            user_id=user_id,
                            user_cache=user_cache,
                            company_cache=company_cache
                        )
                    
                        existing_query = db.query(AnnualPrediction).filter(
                            AnnualPrediction.company_id == company.id,
                            AnnualPrediction.reporting_year == str(row['reporting_year'])
                        )
                        
                        if row.get('reporting_quarter'):
                            existing_query = existing_query.filter(
                                AnnualPrediction.reporting_quarter == row['reporting_quarter']
                            )
                        else:
                            existing_query = existing_query.filter(
                                AnnualPrediction.reporting_quarter.is_(None)
                            )
                
                        existing_prediction = existing_query.first()
                        if existing_prediction:
                            failed_rows += 1
                            error_details.append({
                                'row': i + 1,
                                'error': f"Prediction already exists for {row['company_symbol']} {row['reporting_year']}"
                            })
                            # Log duplicate skip for debugging
                            task_logger.warning(
                                f"âš ï¸ Skipping duplicate prediction for {row['company_symbol']} {row['reporting_year']}",
                                job_id=job_id,
                                row_number=i+1,
                                company_symbol=row['company_symbol'],
                                reporting_year=row['reporting_year']
                            )
                            continue
                        
                        financial_data = {
                            'long_term_debt_to_total_capital': safe_float(row['long_term_debt_to_total_capital']),
                            'total_debt_to_ebitda': safe_float(row['total_debt_to_ebitda']),
                            'net_income_margin': safe_float(row['net_income_margin']),
                            'ebit_to_interest_expense': safe_float(row['ebit_to_interest_expense']),
                            'return_on_assets': safe_float(row['return_on_assets'])
                        }
                        
                        ml_result = ml_model.predict_default_probability(financial_data)
                        
                        if organization_id:
                            access_level = "organization"
                        else:
                            user = db.query(User).filter(User.id == user_id).first()
                            if user and user.role == "super_admin":
                                access_level = "system"
                            else:
                                access_level = "personal"
                        
                        prediction = AnnualPrediction(
                            id=uuid.uuid4(),
                            company_id=company.id,
                            organization_id=organization_id,
                            access_level=access_level,
                            reporting_year=str(row['reporting_year']),
                            reporting_quarter=row.get('reporting_quarter') if row.get('reporting_quarter') else None,
                            long_term_debt_to_total_capital=safe_float(row['long_term_debt_to_total_capital']),
                            total_debt_to_ebitda=safe_float(row['total_debt_to_ebitda']),
                            net_income_margin=safe_float(row['net_income_margin']),
                            ebit_to_interest_expense=safe_float(row['ebit_to_interest_expense']),
                            return_on_assets=safe_float(row['return_on_assets']),
                            probability=safe_float(ml_result['probability']),
                            risk_level=ml_result['risk_level'],
                            confidence=safe_float(ml_result['confidence']),
                            predicted_at=datetime.utcnow(),
                            created_by=user_id
                        )
                        
                        db.add(prediction)
                        successful_rows += 1
                        
                        if (i + 1) % 50 == 0:
                            db.commit()
                            update_job_status(
                                job_id, 
                                'processing',
                                processed_rows=i + 1,
                                successful_rows=successful_rows,
                                failed_rows=failed_rows
                            )
                            
                            self.update_state(
                                state="PROGRESS",
                                meta={
                                    "status": f"Processed {i + 1}/{total_rows} rows",
                                    "current": i + 1,
                                    "total": total_rows,
                                    "successful": successful_rows,
                                    "failed": failed_rows,
                                    "job_id": job_id
                                }
                            )
                            
                    except Exception as e:
                        failed_rows += 1
                        error_details.append({
                            'row': i + 1,
                            'data': {k: str(v) for k, v in row.items()},  
                            'error': str(e)
                        })
                        
                        # Enhanced error logging for individual rows
                        task_logger.error(
                            f"âŒ Row processing failed: {str(e)}",
                            job_id=job_id,
                            user_id=user_id,
                            file_name=file_name,
                            total_rows=total_rows,
                            processed_rows=i,
                            queue_priority=queue_priority,
                            row_number=i+1,
                            company_symbol=row.get('company_symbol', 'unknown'),
                            error=str(e)
                        )
                        
                        db.rollback()
                        continue
                        
                except Exception as outer_error:
                    # Handle any errors not caught by nested try/except in row processing
                    failed_rows += 1
                    error_details.append({
                        'row': i + 1,
                        'error': f"Outer processing error: {str(outer_error)}"
                    })
                    
                    task_logger.error(
                        f"âŒ Outer processing error on row {i+1}: {str(outer_error)}",
                        job_id=job_id,
                        user_id=user_id,
                        row_number=i+1,
                        error=str(outer_error)
                    )
                    
                    db.rollback()
                    continue
        
            # Final commit and completion logging
            db.commit()
            
            processing_time = time.time() - start_time
            rows_per_second = total_rows / processing_time if processing_time > 0 else 0
            success_rate = (successful_rows / total_rows * 100) if total_rows > 0 else 0
            
            # Log detailed completion statistics
            task_logger.success(
                f"ðŸŽ‰ Bulk upload completed successfully",
                job_id=job_id,
                user_id=user_id,
                file_name=file_name,
                total_rows=total_rows,
                processed_rows=total_rows,
                queue_priority=queue_priority,
                successful_rows=successful_rows,
                failed_rows=failed_rows,
                success_rate_percent=success_rate,
                processing_time_seconds=processing_time,
                rows_per_second=rows_per_second
            )
            
            # Log final summary for debugging
            task_logger.success(
                f"ðŸŽ‰ Job completion summary: Processed {total_rows} rows, "
                f"Created {successful_rows} predictions, "
                f"Skipped {failed_rows} duplicates/errors",
                job_id=job_id,
                user_id=user_id,
                total_rows=total_rows,
                successful_rows=successful_rows,
                failed_rows=failed_rows,
                success_rate_percent=success_rate,
                processing_time_seconds=processing_time
            )
            
            update_job_status(
                job_id,
                'completed',
                processed_rows=total_rows,
                successful_rows=successful_rows,
                failed_rows=failed_rows,
                error_details={'errors': error_details[:100]}  
            )
        
        except Exception as processing_error:
            # Handle any errors during the main processing loop
            task_logger.error(
                f"âŒ Main processing loop failed: {str(processing_error)}",
                job_id=job_id,
                user_id=user_id,
                error=str(processing_error)
            )
            
            # Update job status to failed
            update_job_status(
                job_id,
                'failed',
                processed_rows=len(data) if 'data' in locals() else 0,
                successful_rows=successful_rows if 'successful_rows' in locals() else 0,
                failed_rows=failed_rows if 'failed_rows' in locals() else 0,
                error_message=str(processing_error)
            )
            
            raise processing_error  # Re-raise to be caught by function-level except
        
        result = {
            "status": "completed",
            "job_id": job_id,
            "total_rows": total_rows,
            "successful_rows": successful_rows,
            "failed_rows": failed_rows,
            "processing_time_seconds": round(processing_time, 2),
            "rows_per_second": round(rows_per_second, 2),
            "success_rate_percent": round(success_rate, 2),
            "errors": error_details[:10]  
        }
        
        return result
        
    except Exception as e:
        db.rollback()
        error_msg = str(e)
        processing_time = time.time() - start_time
        
        # Enhanced error logging for task failures
        task_logger.error(
            f"ðŸ’¥ CRITICAL: Task failed with exception",
            job_id=job_id,
            user_id=user_id,
            file_name=file_name,
            total_rows=total_rows,
            processed_rows=successful_rows + failed_rows,
            queue_priority=queue_priority,
            error=error_msg,
            processing_time_seconds=processing_time
        )
        
        # Log full stack trace for debugging
        task_logger.error(
            f"ðŸ“‹ Full stack trace: {traceback.format_exc()}",
            job_id=job_id,
            user_id=user_id
        )
        
        logger.error(f"Bulk upload job {job_id} failed: {error_msg}")
        logger.error(f"Full traceback: {traceback.format_exc()}")
        
        # Use empty list if error_details is not defined yet
        error_list = locals().get('error_details', [])
        
        update_job_status(
            job_id,
            'failed',
            error_message=error_msg,
            error_details={'errors': error_list, 'exception': traceback.format_exc()}
        )
        
        self.update_state(
            state="FAILURE",
            meta={
                "status": f"Job failed: {error_msg}",
                "job_id": job_id,
                "error": error_msg,
                "processing_time_seconds": round(processing_time, 2),
                "exc_type": type(e).__name__,
                "exc_message": error_msg
            }
        )
        
        return {
            "status": "failed",
            "job_id": job_id,
            "error": error_msg,
            "total_rows": total_rows,
            "successful_rows": successful_rows,
            "failed_rows": failed_rows
        }
    
    finally:
        db.close()


# PERFORMANCE OPTIMIZATION: Add caching for database lookups
def create_or_get_company_cached(db, symbol: str, name: str, market_cap: float, sector: str, 
                                organization_id: Optional[str], user_id: str, 
                                user_cache: dict, company_cache: dict) -> Company:
    """Optimized version with caching to avoid repeated DB queries"""
    cache_key = f"{symbol}_{organization_id or 'none'}_{user_id}"
    
    # Check company cache first
    if cache_key in company_cache:
        company = company_cache[cache_key]
        # Update company data
        company.name = name
        company.market_cap = safe_float(market_cap) * 1_000_000
        company.sector = sector
        return company
    
    # Check user cache
    if user_id not in user_cache:
        user = db.query(User).filter(User.id == user_id).first()
        user_cache[user_id] = user
    else:
        user = user_cache[user_id]
    
    # Determine access level
    if organization_id:
        access_level = "organization"
        company = db.query(Company).filter(
            Company.symbol == symbol.upper(),
            Company.organization_id == organization_id,
            Company.access_level == "organization"
        ).first()
    else:
        if user and user.role == "super_admin":
            access_level = "system"
            company = db.query(Company).filter(
                Company.symbol == symbol.upper(),
                Company.access_level == "system"
            ).first()
        else:
            access_level = "personal"
            company = db.query(Company).filter(
                Company.symbol == symbol.upper(),
                Company.organization_id.is_(None),
                Company.access_level == "personal",
                Company.created_by == user_id
            ).first()
    
    if not company:
        company = Company(
            symbol=symbol.upper(),
            name=name,
            market_cap=safe_float(market_cap) * 1_000_000,
            sector=sector,
            organization_id=organization_id,
            access_level=access_level,
            created_by=user_id
        )
        db.add(company)
        db.flush()
    else:
        company.name = name
        company.market_cap = safe_float(market_cap) * 1_000_000
        company.sector = sector
    
    # Cache the result
    company_cache[cache_key] = company
    return company


@celery_app.task(bind=True, name="app.workers.tasks.process_quarterly_bulk_upload_task")
@enhanced_task_logging("process_quarterly_bulk_upload_task")
def process_quarterly_bulk_upload_task(
    self, 
    job_id: str, 
    data: List[Dict[str, Any]], 
    user_id: str, 
    organization_id: Optional[str]
) -> Dict[str, Any]:
    """
    Enhanced Celery task to process quarterly predictions bulk upload with comprehensive logging
    Enhanced Celery task to process quarterly predictions bulk upload with comprehensive logging
    
    Args:
        job_id: Bulk upload job ID
        data: List of row data from Excel/CSV
        user_id: ID of user who initiated upload
        organization_id: Organization ID (None for super admin global uploads)
        
    Returns:
        Dictionary with processing results
    """
    task_id = self.request.id
    start_time = time.time()
    
    # Initialize enhanced logger
    task_logger = TaskLogger("process_quarterly_bulk_upload_task")
    
    # Initialize error tracking
    error_details = []
    successful_rows = 0
    failed_rows = 0
    
    # Get job details for logging
    SessionLocal = get_session_local()
    db = SessionLocal()
    
    try:
        job = db.query(BulkUploadJob).filter(BulkUploadJob.id == job_id).first()
        file_name = job.original_filename if job else "unknown-file"
        total_rows = len(data)
        
        # Determine queue priority based on file size
        if total_rows <= 100:
            queue_priority = "high"
        elif total_rows <= 1000:
            queue_priority = "medium"
        else:
            queue_priority = "low"
        
        # Log detailed task start
        task_logger.log_task_start(
            f"ðŸ“Š Starting quarterly bulk upload processing",
            job_id=job_id,
            user_id=user_id,
            file_name=file_name,
            total_rows=total_rows,
            processed_rows=0,
            queue_priority=queue_priority,
            successful_rows=0,
            failed_rows=0,
            success_rate_percent=0,
            processing_time_seconds=0,
            rows_per_second=0
        )
        
        update_job_status(job_id, 'processing')
        
        self.update_state(
            state="PROGRESS",
            meta={
                "status": "Processing quarterly predictions...",
                "current": 0,
                "total": total_rows,
                "job_id": job_id
            }
        )
        
        for i, row in enumerate(data):
            try:
                company = create_or_get_company(
                    db=db,
                    symbol=row['company_symbol'],
                    name=row['company_name'],
                    market_cap=safe_float(row['market_cap']),
                    sector=row['sector'],
                    organization_id=organization_id,
                    user_id=user_id
                )
                
                existing_prediction = db.query(QuarterlyPrediction).filter(
                    QuarterlyPrediction.company_id == company.id,
                    QuarterlyPrediction.reporting_year == str(row['reporting_year']),
                    QuarterlyPrediction.reporting_quarter == row['reporting_quarter']
                ).first()
                
                if existing_prediction:
                    failed_rows += 1
                    error_details.append({
                        'row': i + 1,
                        'error': f"Prediction already exists for {row['company_symbol']} {row['reporting_year']} {row['reporting_quarter']}"
                    })
                    continue
                
                financial_data = {
                    'total_debt_to_ebitda': safe_float(row['total_debt_to_ebitda']),
                    'sga_margin': safe_float(row['sga_margin']),
                    'long_term_debt_to_total_capital': safe_float(row['long_term_debt_to_total_capital']),
                    'return_on_capital': safe_float(row['return_on_capital'])
                }
                
                ml_result = quarterly_ml_model.predict_quarterly_default_probability(financial_data)
                
                if organization_id:
                    access_level = "organization"
                else:
                    user = db.query(User).filter(User.id == user_id).first()
                    if user and user.role == "super_admin":
                        access_level = "system"
                    else:
                        access_level = "personal"
                
                prediction = QuarterlyPrediction(
                    id=uuid.uuid4(),
                    company_id=company.id,
                    organization_id=organization_id,
                    access_level=access_level,
                    reporting_year=str(row['reporting_year']),
                    reporting_quarter=row['reporting_quarter'],
                    total_debt_to_ebitda=safe_float(row['total_debt_to_ebitda']),
                    sga_margin=safe_float(row['sga_margin']),
                    long_term_debt_to_total_capital=safe_float(row['long_term_debt_to_total_capital']),
                    return_on_capital=safe_float(row['return_on_capital']),
                    logistic_probability=safe_float(ml_result.get('logistic_probability', 0)),
                    gbm_probability=safe_float(ml_result.get('gbm_probability', 0)),
                    ensemble_probability=safe_float(ml_result.get('ensemble_probability', 0)),
                    risk_level=ml_result['risk_level'],
                    confidence=safe_float(ml_result['confidence']),
                    predicted_at=datetime.utcnow(),
                    created_by=user_id
                )
                
                db.add(prediction)
                successful_rows += 1
                
                # Enhanced progress logging every 7 rows or at specific intervals
                if (i + 1) % 7 == 0 or (i + 1) in [1, 5, 10, 15, 20] or (i + 1) == total_rows:
                    current_time = time.time()
                    processing_time = current_time - start_time
                    rows_per_second = (i + 1) / processing_time if processing_time > 0 else 0
                    progress_percent = ((i + 1) / total_rows) * 100 if total_rows > 0 else 0
                    success_rate = (successful_rows / (i + 1)) * 100 if (i + 1) > 0 else 0
                    
                    # Database commit for progress updates
                    db.commit()
                    update_job_status(
                        job_id, 
                        'processing',
                        processed_rows=i + 1,
                        successful_rows=successful_rows,
                        failed_rows=failed_rows
                    )
                    
                    # Enhanced progress logging
                    task_logger.log_progress(
                        f"ðŸ“ˆ Processing progress: {progress_percent:.1f}% ({i + 1}/{total_rows} rows)",
                        job_id=job_id,
                        user_id=user_id,
                        file_name=file_name,
                        total_rows=total_rows,
                        processed_rows=i + 1,
                        queue_priority=queue_priority,
                        successful_rows=successful_rows,
                        failed_rows=failed_rows,
                        success_rate_percent=success_rate,
                        processing_time_seconds=processing_time,
                        rows_per_second=rows_per_second
                    )
                    
                    self.update_state(
                        state="PROGRESS",
                        meta={
                            "status": f"Processed {i + 1}/{total_rows} rows",
                            "current": i + 1,
                            "total": total_rows,
                            "successful": successful_rows,
                            "failed": failed_rows,
                            "job_id": job_id
                        }
                    )
                elif (i + 1) % 50 == 0:  # Fallback for larger files
                    db.commit()
                    update_job_status(
                        job_id, 
                        'processing',
                        processed_rows=i + 1,
                        successful_rows=successful_rows,
                        failed_rows=failed_rows
                    )
                    
            except Exception as row_error:
                failed_rows += 1
                error_details.append({
                    'row': i + 1,
                    'data': {k: str(v) for k, v in row.items()},  
                    'error': str(row_error)
                })
                
                db.rollback()
                continue
            
        # Final commit and completion logging
        db.commit()
        
        processing_time = time.time() - start_time
        rows_per_second = total_rows / processing_time if processing_time > 0 else 0
        success_rate = (successful_rows / total_rows) * 100 if total_rows > 0 else 0
        
        # Enhanced completion logging
        task_logger.log_completion(
            f"ðŸŽ‰ Bulk upload completed successfully",
            job_id=job_id,
            user_id=user_id,
            file_name=file_name,
            total_rows=total_rows,
            processed_rows=total_rows,
            queue_priority=queue_priority,
            successful_rows=successful_rows,
            failed_rows=failed_rows,
            success_rate_percent=success_rate,
            processing_time_seconds=processing_time,
            rows_per_second=rows_per_second
        )
        
        update_job_status(
            job_id,
            'completed',
            processed_rows=total_rows,
            successful_rows=successful_rows,
            failed_rows=failed_rows,
            error_details={'errors': error_details[:100]}  
        )
        
        result = {
            "status": "completed",
            "job_id": job_id,
            "total_rows": total_rows,
            "successful_rows": successful_rows,
            "failed_rows": failed_rows,
            "processing_time_seconds": round(processing_time, 2),
            "rows_per_second": round(rows_per_second, 2),
            "success_rate_percent": round(success_rate, 2),
            "errors": error_details[:10]  
        }
        
        return result
            
    except Exception as e:
        db.rollback()
        error_msg = str(e)
        logger.error(f"Bulk upload job {job_id} failed: {error_msg}")
        
        # Use empty list if error_details is not defined yet
        error_list = locals().get('error_details', [])
        
        update_job_status(
            job_id,
            'failed',
            error_message=error_msg,
            error_details={'errors': error_list}
        )
        
        self.update_state(
            state="FAILURE",
            meta={
                "status": f"Task failed: {error_msg}",
                "job_id": job_id,
                "error": error_msg,
                "exc_type": type(e).__name__,
                "exc_message": error_msg
            }
        )
        
        return {
            "status": "failed",
            "job_id": job_id,
            "error": error_msg,
            "total_rows": len(data) if 'data' in locals() else 0,
            "successful_rows": 0,
            "failed_rows": 0,
            "processing_time_seconds": round(time.time() - start_time, 2)
        }
            
    finally:
        db.close()


@celery_app.task(bind=True, name="app.workers.tasks.process_bulk_excel_task")
@enhanced_task_logging("process_bulk_excel_task")
def process_bulk_excel_task(self, file_content_b64: str, original_filename: str) -> Dict[str, Any]:
    """
    Background task to process Excel file with multiple companies for bulk predictions.
    
    Args:
        file_content_b64: Base64 encoded content of the uploaded Excel file
        original_filename: Original name of the uploaded file
        
    Returns:
        Dictionary with processing results
    """
    task_id = self.request.id
    start_time = time.time()
    
    self.update_state(
        state="PROGRESS",
        meta={
            "status": "Starting bulk prediction processing...",
            "current": 0,
            "total": 0,
            "filename": original_filename
        }
    )
    
    session_factory = get_session_local()
    db = session_factory()
    
    results = []
    error_details = []
    successful_predictions = 0
    failed_predictions = 0
    total_companies = 0
    error_details = []  # Initialize error_details list
    
    try:
        file_content = base64.b64decode(file_content_b64)
        df = pd.read_excel(io.BytesIO(file_content))
        total_companies = len(df)
        
        # Process each company...
        for i, row in df.iterrows():
            try:
                company_symbol = row.get('stock_symbol')
                company_name = row.get('company_name')
                market_cap = row.get('market_cap')
                sector = row.get('sector')
                
                # Basic validation
                if not company_symbol or not company_name:
                    failed_predictions += 1
                    error_details.append({
                        'row': i + 1,
                        'error': "Missing required fields: 'stock_symbol' or 'company_name'"
                    })
                    continue
                
                # Create or get company
                company = create_or_get_company(
                    db=db,
                    symbol=company_symbol,
                    name=company_name,
                    market_cap=market_cap,
                    sector=sector,
                    organization_id=None,  # Global scope for bulk upload
                    user_id="system"  # System user for bulk uploads
                )
                
                # For each financial metric, predict default probability
                financial_metrics = ['long_term_debt_to_total_capital', 'total_debt_to_ebitda', 
                                    'net_income_margin', 'ebit_to_interest_expense', 'return_on_assets']
                
                for metric in financial_metrics:
                    if metric not in row or row[metric] is None:
                        failed_predictions += 1
                        error_details.append({
                            'row': i + 1,
                            'error': f"Missing value for financial metric: {metric}"
                        })
                        break
                
                # Predict using ML model
                financial_data = {metric: safe_float(row[metric]) for metric in financial_metrics}
                ml_result = ml_model.predict_default_probability(financial_data)
                
                # Save prediction result
                prediction = AnnualPrediction(
                    id=uuid.uuid4(),
                    company_id=company.id,
                    organization_id=None,  # Global scope
                    access_level="system",  # System access for bulk uploads
                    reporting_year=str(row['reporting_year']),
                    reporting_quarter=row.get('reporting_quarter'),
                    long_term_debt_to_total_capital=safe_float(row['long_term_debt_to_total_capital']),
                    total_debt_to_ebitda=safe_float(row['total_debt_to_ebitda']),
                    net_income_margin=safe_float(row['net_income_margin']),
                    ebit_to_interest_expense=safe_float(row['ebit_to_interest_expense']),
                    return_on_assets=safe_float(row['return_on_assets']),
                    probability=safe_float(ml_result['probability']),
                    risk_level=ml_result['risk_level'],
                    confidence=safe_float(ml_result['confidence']),
                    predicted_at=datetime.utcnow(),
                    created_by="system"  # System user
                )
                
                db.add(prediction)
                successful_predictions += 1
                
                # Commit in batches
                if (i + 1) % 50 == 0:
                    db.commit()
                    self.update_state(
                        state="PROGRESS",
                        meta={
                            "status": f"Processed {i + 1}/{total_companies} companies",
                            "current": i + 1,
                            "total": total_companies,
                            "successful": successful_predictions,
                            "failed": failed_predictions,
                            "job_id": task_id
                        }
                    )
            
            except Exception as e:
                failed_predictions += 1
                error_details.append({
                    'row': i + 1,
                    'data': {k: str(v) for k, v in row.items()},  
                    'error': str(e)
                })
                
                db.rollback()
                continue
        
        # Final commit
        db.commit()
        
        processing_time = time.time() - start_time
        
        self.update_state(
            state="SUCCESS",
            meta={
                'status': 'Bulk processing completed',
                'total_companies': total_companies,
                'successful_predictions': successful_predictions,
                'failed_predictions': failed_predictions,
                'processing_time': processing_time
            }
        )
        
        return {
            "success": True,
            "message": f"Processed {total_companies} companies successfully",
            "filename": original_filename,
            "total_companies": total_companies,
            "successful_predictions": successful_predictions,
            "failed_predictions": failed_predictions,
            "results": results,
            "processing_time": round(processing_time, 2),
            "completed_at": time.time()
        }
        
    except Exception as e:
        db.rollback()
        error_message = str(e)
        error_traceback = traceback.format_exc()
        
        print(f"Bulk prediction task failed: {error_message}")
        print(f"Traceback: {error_traceback}")
        
        self.update_state(
            state="FAILURE",
            meta={
                "error": error_message,
                "traceback": error_traceback,
                "filename": original_filename,
                "processed": successful_predictions + failed_predictions,
                "exc_type": type(e).__name__,
                "exc_message": error_message
            }
        )
        
        raise type(e)(error_message)
        
    finally:
        try:
            db.close()
        except Exception:
            pass


@celery_app.task(name="app.workers.tasks.process_quarterly_bulk_task")
@enhanced_task_logging("process_quarterly_bulk_task")
def process_quarterly_bulk_task(file_content_b64: str, original_filename: str, organization_id: str = None, created_by: str = None):
    """
    Process quarterly bulk prediction file in background
    """
    start_time = time.time()
    SessionLocal = get_session_local()
    db = SessionLocal()
    
    try:
        current_task.update_state(
            state='PROGRESS',
            meta={'status': 'Processing quarterly file...', 'progress': 0}
        )
        
        file_content = base64.b64decode(file_content_b64)
        
        if original_filename.endswith('.csv'):
            df = pd.read_csv(io.StringIO(file_content.decode('utf-8')))
        else:
            df = pd.read_excel(io.BytesIO(file_content))
        
        total_companies = len(df)
        
        results = []
        successful_predictions = 0
        failed_predictions = 0
        
        for index, row in df.iterrows():
            try:
                progress = int((index / total_companies) * 100)
                current_task.update_state(
                    state='PROGRESS',
                    meta={
                        'status': f'Processing company {index + 1}/{total_companies}',
                        'progress': progress,
                        'current_company': row.get('stock_symbol', 'Unknown')
                    }
                )
                
                # Process individual quarterly prediction
                company_symbol = row.get('stock_symbol')
                company_name = row.get('company_name')
                market_cap = row.get('market_cap')
                sector = row.get('sector')
                
                # Create or get company
                company = create_or_get_company(
                    db=db,
                    symbol=company_symbol,
                    name=company_name,
                    market_cap=market_cap,
                    sector=sector,
                    organization_id=organization_id,
                    user_id=created_by
                )
                
                # Check if prediction already exists
                existing_prediction = db.query(QuarterlyPrediction).filter(
                    QuarterlyPrediction.company_id == company.id,
                    QuarterlyPrediction.reporting_year == str(row['reporting_year']),
                    QuarterlyPrediction.reporting_quarter == row['reporting_quarter']
                ).first()
                
                if existing_prediction:
                    failed_predictions += 1
                    results.append({
                        "stock_symbol": company_symbol,
                        "company_name": company_name,
                        "success": False,
                        "error": f"Prediction already exists for {row['reporting_year']} Q{row['reporting_quarter']}"
                    })
                    continue
                
                # Prepare financial data for prediction
                financial_data = {
                    'total_debt_to_ebitda': safe_float(row['total_debt_to_ebitda']),
                    'sga_margin': safe_float(row['sga_margin']),
                    'long_term_debt_to_total_capital': safe_float(row['long_term_debt_to_total_capital']),
                    'return_on_capital': safe_float(row['return_on_capital'])
                }
                
                # Predict using ML model
                ml_result = quarterly_ml_model.predict_quarterly_default_probability(financial_data)
                
                # Save prediction result
                prediction = QuarterlyPrediction(
                    id=uuid.uuid4(),
                    company_id=company.id,
                    organization_id=organization_id,
                    access_level="organization" if organization_id else "personal",
                    reporting_year=str(row['reporting_year']),
                    reporting_quarter=row['reporting_quarter'],
                    total_debt_to_ebitda=safe_float(row['total_debt_to_ebitda']),
                    sga_margin=safe_float(row['sga_margin']),
                    long_term_debt_to_total_capital=safe_float(row['long_term_debt_to_total_capital']),
                    return_on_capital=safe_float(row['return_on_capital']),
                    logistic_probability=safe_float(ml_result.get('logistic_probability', 0)),
                    gbm_probability=safe_float(ml_result.get('gbm_probability', 0)),
                    ensemble_probability=safe_float(ml_result.get('ensemble_probability', 0)),
                    risk_level=ml_result['risk_level'],
                    confidence=safe_float(ml_result['confidence']),
                    predicted_at=datetime.utcnow(),
                    created_by=created_by
                )
                
                db.add(prediction)
                successful_predictions += 1
                
            except Exception as e:
                failed_predictions += 1
                results.append({
                    "stock_symbol": row.get('stock_symbol', 'Unknown'),
                    "company_name": row.get('company_name', 'Unknown'),
                    "success": False,
                    "error": f"Processing error: {str(e)}"
                })
                failed_predictions += 1
        
        current_task.update_state(
            state='SUCCESS',
            meta={
                'status': 'Quarterly bulk processing completed',
                'progress': 100,
                'total_processed': total_companies,
                'successful_predictions': successful_predictions,
                'failed_predictions': failed_predictions,
                'processing_time': time.time() - start_time
            }
        )
        
        return {
            "success": True,
            "message": f"Quarterly bulk processing completed. Success: {successful_predictions}, Failed: {failed_predictions}",
            "total_processed": total_companies,
            "successful_predictions": successful_predictions,
            "failed_predictions": failed_predictions,
            "results": results,
            "processing_time": time.time() - start_time,
            "prediction_type": "quarterly",
            "filename": original_filename
        }
        
    except Exception as e:
        error_msg = f"Quarterly bulk processing failed: {str(e)}"
        print(f"Error in quarterly bulk task: {error_msg}")
        print(f"Traceback: {traceback.format_exc()}")
        
        current_task.update_state(
            state='FAILURE',
            meta={'error': error_msg}
        )
        
        return {
            "success": False,
            "message": error_msg,
            "total_processed": 0,
            "successful_predictions": 0,
            "failed_predictions": 0,
            "results": [],
            "processing_time": time.time() - start_time,
            "error": error_msg,
            "prediction_type": "quarterly",
            "filename": original_filename
        }
    
    finally:
        db.close()
